# Lab 5 - Localization
## Introduction
Determining a robot’s orientation and position in a known environment, also known as localization, is a critical problem in the field of robotics. As is common in robotics, this seemingly simple problem is surprisingly difficult, and remains an active research area. In this lab, you will solve robotic localization by implementing Monte Carlo Localization (aka MCL, particle filter, or recursive bayes filter).

This lab consists of three parts:

-   **Part A** - Understand the mathematical backing behind particle filter
-   **Part B** - Develop and test the particle filter algorithm in simulation 
-   **Part C** - Demonstrate the functionality of your algorithm on the real RACECAR; configure and run **Google Cartographer** on the real RACECAR

## Submission 

**Part A** - Submit your solutions to writing assignment on **stellar**. 

**Part B** - Submit your solutions to programming assignment on **gradescope**. After running the automated tests described at the  end of this 			lab, you will generate a file called  `log.npz`.  **You must upload this to gradescope for credit**.

Part A & B are due 1 week from the release date on  **March 20, 2019**.

**Part C** - Run your implementations in part B on real RACECAR and present your results to TAs. The things we will look into:
   -   Numerical evidence that your algorithm is working in the form of charts/data
		-   Convergence rates, cross track error, etc
		-   The simulator odometry is perfectly accurate, so that can be your ground truth!
	-   An [illustrative video](https://www.youtube.com/watch?v=-c_0hSjgLYw&t=6s) of your particle filter working in a simulation environment, overlaying:
		-  Visualization of inferred position
		-   Visualization of the particle distribution
		-   The known map
		-   Laser scan data in the coordinate frame of your inferred position (it should align fairly well with the walls in the known map)
		-   Any other byproducts of your algorithm which you find worthy of visualization
	- A [similarly illustrative video of your car localizing](https://www.youtube.com/watch?v=-c_0hSjgLYw) in the Stata basement environment

This lab will **be presented alongside lab 6** which is due on **April 3, 2019**.

## Part A - Writing Assignment
Work on the following questions and submit your answers to stellar. These questions will help you understand the algorithm before diving into coding.
1. (**Motion Model**) Consider a deterministic motion model based on odometry information. The motion model takes as arguments the old particle pose, as well as the last odometry data, and return a new pose with the odometry “applied” to the old poses. 
$$
\Delta \mathbf{x}:=[\Delta x, \Delta y, \Delta\theta]^T=f(u_t)\\
\mathbf{x}_t = g(\mathbf{x}_{t-1}, \Delta x)
$$
where $f(\cdot)$  takes the control input $u_t$ and returns the changes in pose, $g(\mathbf{x}_{t-1}, \Delta x)$ is the motion model applying the observed change $\Delta x, \Delta y, \Delta \theta$ in pose to the previous robot's pose $\mathbf{x}_{t-1}$ to get predictions of the next pose $\mathbf{x}_t$. 

	i. Let the poses estimated by odometry are $\mathbf{\hat{x}}_{t-1} = [0, 0, \pi/6]^T$ and $\mathbf{\hat{x}}_{t} = [0.2, 0.1, 11\pi/6]^T$ at time $t-1$ and $t$, respectively. Both are expressed in the world coordinate system. Find the estimated change of pose $\Delta \mathbf{x}$ between time $t-1$ and $t$ expressed in the body frame.

	ii. Odometry data is accumulated via dead reckoning, therefore it is very inaccurate. Now assume the previous pose estimated by the particle filter is $\mathbf{x}_{t-1} = [3,4,\pi/3]^T$ (in world frame). Find current pose $\mathbf{x}_t$ (in world frame) estimated based on the measurement from odometry in part (a).

2. (**Sensor Model**) The sensor model $p(z_{t}^{k}| x_{t}, m)$ defines how likely it is to record a given sensor reading $z_{t}^{k}$ from a hypothesis position $x_{t}$ in the map $m$ at time $t$. The definition of this likelihood is strongly dependent on the type of sensor used - a laser scanner in our case.

   Typically, there are a few cases to be modeled in determining $p(z_{t}^{k}| x_{t}, m)$:

	a.  Probability of detecting a known obstacle in the map
    b.  Probability of a short measurement, maybe due to unknown obstacles (cats, people, etc)
    c.  Probability of a missed measurement, usually due to a reflected LiDAR beam
    d.  Probability of a random measurement, maybe due to unexpected asteroid collisions
    
	We typically represent (a) as a gaussian distribution around the ground truth distance between the hypothesis pose and the known map obstacle. Thus, if the measured range exactly matches the expected range, the probability is large. If the measured range is $z_{t}^{k}$ and the ground truth range is determined (via ray casting) to be $z_{t}^{k^{\ast}}$:

	$$
	p_{hit}(z_{t}^{k}| x_{t}, m)  = \begin{cases}
	\frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(z_{t}^{k}-z_{t}^{k^{\ast}})^2}{2\sigma^2}}, & \text{if}~ 0 \leq z_{t}^{k} \leq z_{t}^{k^{\ast}}\\
	0, & \text{otherwise} 
	\end{cases}
	$$
	
	(b) is represented as a downward sloping line as the ray gets further from the robot. Intuitively, the likelihood of sensing unexpected objects decreases with range because we are more likely to detect obstacles nearby. This likelihood can be modeled as:
	$$
	p_{short}(z_{t}^{k}| x_{t}, m) =  \begin{cases}
         \lambda_{short} (1 - \frac{z_{t}^{k}}{z_{t}^{k^{\ast}}}),   &   \text{if}~ 0 \leq z_{t}^{k} \leq z_{t}^{k^{\ast}}\\
         0,   &   \text{otherwise}
\end{cases}
	$$
	(c\) is represented by a large spike in probability at the maximal range value, so that reflected measurements do not significantly discount particle weights.
	$$
	p_{max}(z_{t}^{k}| x_{t}, m)  =\begin{cases}
	1,  &  \text{if}~ z = z_{max}\\
	0,  &  \text{otherwise} 
	\end{cases}
	$$
	(d) is represented by a small uniform value, to account for unforeseen effects.
	$$
	p_{rand}(z_{t}^{k}| x_{t}, m)  = \begin{cases}
	\frac{1}{z_{max}},  &  \text{if}~ 0\leq z_{t}^{k} < z_{max}\\
	0,                            & \text{otherwise} 
	\end{cases}
	$$
	These four different distributions are now mixed by a weighted average, defined by the parameters $\alpha_{hit}, \alpha_{short},\alpha_{max},\alpha_{rand}$:
	 $$
	 p(z_{t}^{k}| x_{t}, m)  = \alpha_{hit} \cdot p_{hit}(z_{t}^{k}| x_{t}, m)  \\+ \alpha_{short} \cdot p_{short}(z_{t}^{k}| x_{t}, m)  \\+ \alpha_{max} \cdot p_{max}(z_{t}^{k}| x_{t}, m)  \\+ \alpha_{rand} \cdot p_{rand}(z_{t}^{k}| x_{t}, m) 
	 $$
	 Given $\alpha_{hit} = 0.75$, $\alpha_{short}=0.01$,$\alpha_{max}=0.07$,$\alpha_{rand}=0.12$, $z_{max}=200$ px, $\sigma=8.0, \lambda_{short}=2.0$ and $z_{t}^{k^{\ast}} = 140$ px, find the probability  $p(z_{t}^{k}| x_{t}, m)$  when
	 i. $~~z_{t}^{k} = 0$
	 ii. $~z_{t}^{k} = 50$
	 iii. $z_{t}^{k} = 100$
	 iv. $z_{t}^{k} = 150$
	 v. $~z_{t}^{k} = 200$
3. Show that the sampling of particle filter is biased for a finite number of samples.

## Part B - Programming Assignment
### 1. Getting Started
Grab (fork) the skeleton code from here: [https://github.mit.edu/2019-RSS/lab5_localization](https://github.mit.edu/2019-RSS/lab5_localization). In this lab, you will use RangeLibc for super fast ray casting. Check out [the repo](https://github.com/kctess5/range_libc) if for some reason you don’t already have it installed.

### 2. Implementation
#### 2.1 Particle Filter
```python
# a single recursive update step of the MCL algorithm

def MCL(Xt-1,at-1,o):
    Xt = {}
	for i in range(m):
	# sample a pose from the old particles according to old weights. Samples
	# implicitly represent the prior prob. dist. Bel(x_{t-1}) in eqn. (3)
	```math
	x^{i}_{t-1} \sim X_{t-1}
	```

	# update the sampled pose according to the motion model
	xit  p(xt | xt-1, at-1)

	# weight the updated pose according to the sensor model
	wit= p(o | xit )

	# add the new pose and weight to the new distribution
	Xt=Xt{ (xit,wit) }

	# normalize weights, should sum to 1
	Xt=normalize(Xt)

	return Xt

# iterative application of the MCL algorithm
def particle_filter():
	X = Bel(xo) initial particles
	while true:
		a = get_last_odometry()
		o = get_last_sensor_readings()
		X = MCL(X,a,o)

	# inferred pose ← expected value over particle distribution
	pose = Ex[X]
```
#### 2.2 Motion Model
#### 2.3 Sensor Model


### 3. **Test your implementation in simulation**.
 Similar to previous labs, we have provided an autograder to test your particle filter. The autograder will launch the following nodes:
-      racecar/launch/teleop.launch (temporarily at startup)
-   	lab5_localization/launch/map_server.launch
-   	lab5_localization/launch/localize.launch
    

Once the autograder detects your particle filter is running (as soon as it receives a message from `/pf/pose/odom`) it will launch a rosbag collected on a Hokuyo flavored RACECAR while driving around the basement. Then, the autograder receives messages published on `/pf/pose/odom` and extracts (time, x, y, theta) information from each message via `Odometry.pose.pose.position`, `quaternion_to_angle` (`Odometry.pose.pose.orientation`) and `Odometry.header.stamp`. When the test completes (or is terminated early) the autograder will output a `log.npz` file that contains this trajectory data, which you need to upload to Gradescope. You should all submit the log.npz file, but you can share that file among your own team.

Periodically throughout testing, we provide initialization via the `/initialpose `topic, so it’s extremely important that you implement this functionality in your solution. You are not allowed to manually provide your own reinitialization messages during testing.

You may run the test at less than real time via the` --rate` argument (e.g. `./run_tests --rate 0.5`). However, the grade you receive will also be scaled by this quantity according to the below chart, so it is in your interest to run at real time! For fairness, we require that all teams run the test scripts on their car. This will avoid potentially penalizing people with slower laptops.

FYI, our solution generally gets $>0.9$ points on Gradescope. Note: the bit where the car is by the bikes is tricky! The map isn’t perfect, and therefore you will not achieve perfection there. Don’t worry about it, everyone will have the same problem.

Note: you will have to install the map server node on the RACECAR.

	sudo apt-get install ros-kinetic-map-server

### 4. Notes and Tips
#### 4.1 Writing efficient Python
Since the algorithm must run at >20Hz with a large number of particles, an efficient implementation is a requirement for success. There are a few tricks you can use, primarily:
-   Use numpy arrays for absolutely everything - python lists → slow
-   Use numpy functions on numpy arrays to do any computations, 
    - avoid Python for loops like the plague
    -   [Slice indexing is your (best) friend.](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)
-   Use the smallest number of operations required to perform your arithmetic
-   Avoid unnecessary memory allocations
    -   Cache and reuse important numpy arrays by setting them to the right size during initialization of your particle filter as “self” variables
    -   Be careful! Avoid bugs due to stale state in reused variables!
    

-   Identify your critical code paths, and keep them clean
    - Conversely, don’t worry too much about code that is called infrequently
-   Push code to Cython/C++ if necessary
    -   You probably won’t need to do this much since RangeLibc already does this
-   Avoid excessive function calls - function call overhead in Python → slow
-   Use RangeLibc
    -   saveTrace with Bresenham’s Line can be useful for debugging (see docs)
    

-   Don’t publish visualization messages unless someone is subscribed to those topics
```
    if  self.[some_pub].get_num_connections() >  0 …
```
-   USE A PROFILER to identify good candidates for optimization
    -   [http://projects.csail.mit.edu/pr2/wiki/index.php?title=Profiling_Code_in_ROS](http://projects.csail.mit.edu/pr2/wiki/index.php?title=Profiling_Code_in_ROS)
-   Use rostopic hz to determine how fast your nodes are publishing.
    -   If your particle filter is publishing the pose significantly slower than the laser scan than there is a lot of lag.
    

#### 4.2 RangeLibc

Assuming a reasonably efficient architecture, you will find that the bottleneck computation in running your particle filter is the determination of ground truth ranges between hypothesis poses (particles) and the nearest obstacles in the map - ray casting. Rather than having you implement the “calc_range” function yourself in Python, we provide the RangeLibc library to enable real time particle filter operation. See [RangeLibc document](https://docs.google.com/document/d/1NSRAAZhNbmrsyQynYBgQuT2EUYupjg3Qafu1OtkTYtc/export?format=pdf) for an algorithm overview and usage instructions. We highly recommend you read through it since it includes tips to make your sensor model implementations significantly simpler and faster.

#### 4.3 RViz is your friend

-   Visual inspection is the best debugging tool
    

-   Significantly better than looking at a stream of numbers in a terminal
    
-   Add visualization early and often
    

-   Subscribe to the /initialpose or /clicked_point topics and use the associated tools in RViz (“2D Pose Estimate”/”Clicked Point”) to initialize or reinitialize your particle filter [(see video example of this)](https://www.youtube.com/watch?v=IQ1GtNJvBjg)
    

#### 4.4 Debugging tips

#### Motion model

1.  Disable your sensor model update
    
2.  Disable your motion model noise
    
3.  Ensure that when you move the car, the particles move in the same manner.
    
4.  Rotate your car but do not reinitalize your particles - once again drive around and ensure your particles do something reasonable
    
5.  Once you’re happy with the results from the above tests, enable your motion model noise. Now, you should see the particles spreading out as you drive around, but they should all move in roughly the right direction without excessive spread.
    
6.  Enable your sensor model now and see that the particle distribution ‘collapses’ around the correct pose after each sensor measurement is considered
    

#### Sensor model

1. Print your probabilities and look at them. The sensor model is prone to some numerical issues because multiplying a bunch of very small numbers leads to extremely small values which can exceed float precision. If this happens, you might be able to think of ways to rearrange your order of operations to avoid these problems.
    
2. Evaluate your model with custom arguments
    

3. Providing the same ‘observation’ and ‘ground truth’ scans should result in reasonably large probabilities
    
4. Providing two extremely different scans should result in low probabilities
    
5. Etc - make up some test cases and ensure your expectations match reality
    

#### Ray casting with RangeLibc

Try doing some simple cases of ray casting and make sure you get basically what you expect. You can (for example) dump the distance values you get from simulating a laser scan at a known pose, and plot them somehow. What you see in the plot should match your expectations, if not, maybe you have your x/y coordinates reversed, or some other problem like that.
## Part C - Run on Real RACECAR
1. Demonstrate the functionality of your algorithm on the real RACECAR; 
2.  Configure and run **Google Cartographer** on the real RACECAR. [Cartographer](https://github.com/googlecartographer/cartographer) is a system that provides real-time simultaneous localization and mapping ([SLAM](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)) in 2D and 3D across multiple platforms and sensor configurations.  Please go to this repository, follow the instruction to configure and run cartographer on the real RACECAR.

	    https://github.mit.edu/2018-RSS/cartographer_config

We will announce the schedule for  the presentation in next lab.
