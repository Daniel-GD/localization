{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Localization\n",
    "## Introduction\n",
    "(*Open this notebook in Chrome or Firefox to avoid incorrect equation rendering in Safari*)\n",
    "\n",
    "Determining a robot’s orientation and position in a known environment, also known as localization, is a critical problem in the field of robotics. As is common in robotics, this seemingly simple problem is surprisingly difficult, and remains an active research area. In this lab, you will solve robotic localization by implementing Monte Carlo Localization (aka MCL, particle filter, or recursive bayes filter).\n",
    "\n",
    "This lab consists of three parts, the first two of which must be submitted individually:\n",
    "\n",
    "-   **Part A** - (Writing Assignment) Understand the motion and sensor model.\n",
    "-   **Part B** - (Programming Assignment) Develop and test the particle filter algorithm in simulation.  \n",
    "-   **Part C** - Demonstrate the functionality of your algorithm on the real RACECAR. \n",
    "-   **Part D** - (**OPTIONAL**) Configure and run **Google Cartographer** on the real RACECAR.\n",
    "\n",
    "<img src=\"figures/pf.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission \n",
    "<a id='submission'></a>\n",
    "**Part A** - (**INDIVIDUAL EFFORT**) Submit your answers **INDIVIDUALLY** to the writing assignment on **gradescope**, preferably $\\LaTeX$ed. You must show work (derivations, formulas used, etc.) to receive full credit. Just as in most course 6 classes, you may collaborate on problems but you cannot simply share answers. You must write up your solutions independently.\n",
    "\n",
    "Parts A is due at **1 pm, Wednesday, March 20, 2019**, which is 1 week after the release date of this lab.\n",
    "\n",
    "**Part B** - (**TEAMWORK**) Submit your `log.npz` autograder output on **gradescope**. After running the automated tests for your implementation of MCL, the autograder will generate a file called  `log.npz`.  **You must upload this to gradescope to receive credit for part B**.\n",
    "\n",
    "**Part C** - (**TEAMWORK**) Run your implementation of MCL from part B on the real RACECAR and present your results to the teaching staff. The things we will look into:\n",
    "   -   Numerical evidence that your algorithm is working in the form of charts/data\n",
    "\t\t-   Convergence rates, cross track error, etc\n",
    "\t\t-   The simulator odometry is perfectly accurate, so that can be your ground truth!\n",
    "   -   An [illustrative video](https://www.youtube.com/watch?v=-c_0hSjgLYw&t=6s) of your particle filter working in a simulation environment, overlaying:\n",
    "\t\t-  Visualization of inferred position\n",
    "\t\t-   Visualization of the particle distribution\n",
    "\t\t-   The known map\n",
    "\t\t-   Laser scan data in the coordinate frame of your inferred position (it should align fairly well with the walls in the known map)\n",
    "\t\t-   Any other byproducts of your algorithm which you find worthy of visualization\n",
    "   -   A [similarly illustrative video of your car localizing](https://www.youtube.com/watch?v=-c_0hSjgLYw) in the Stata basement environment\n",
    "\n",
    "**Part D** (**TEAMWORK**, *OPTIONAL*) Configure and run Google Cartographer on the real RACECAR.\n",
    "- Build a map of the Stata basement environment using Cartographer\n",
    "- Localize using Cartographer\n",
    "- Show visualization evidence that Cartographer is working on the RACECAR\n",
    "\n",
    "Parts B & C & D are due at **1 pm Wednesday, April 3, 2019**, which is 2 weeks after the release date of this lab.\n",
    "This lab will **be presented alongside lab 6**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading (10+2 points)\n",
    "**Part A**. Writing Assignment (3 points)\n",
    "- 1 point for part (i) in question 1 \n",
    "- 1 point for part (ii) in question 1\n",
    "- 1 point for question 2 \n",
    "\n",
    "**Part B**. Programming implementation (4 points)\n",
    "\n",
    "**Part C**. Lab report, presentation (3 points)\n",
    "\n",
    "**Part D** (**Bonus**). Lab report together with part C, and presentation (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Writing Assignment\n",
    "Work on the following questions, submit your numeric answers to autograder, and justify your answers in your lab report. Just writing the final answer will not give total credit, even if correct. These questions will help you understand the algorithm before diving into coding.\n",
    "\n",
    "<a id='question1'></a>\n",
    "**Question 1**. (**Motion Model**) Consider a deterministic motion model (no added noise) based on odometry information. The motion model, $g$, takes as arguments the old particle pose, $\\mathbf{x}_{t-1}$, as well as the current odometry data, $\\Delta\\mathbf{x}$, and returns a new pose, $\\mathbf{x}_t$ with the odometry “applied” to the old poses:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{x}_t = \\begin{bmatrix}x_t\\\\y_t\\\\\\theta_t\\end{bmatrix} = g(\\mathbf{x}_{t-1}, \\Delta x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the poses $\\mathbf{x}$ are expressed in the world frame relative to some arbitrary origin. The odometry $\\Delta \\mathbf{x}$ is necessarily a local quantity, expressed relative to the previous frame.\n",
    "\n",
    "**i** Suppose you are given $\\mathbf{x}_{t-1} = \\left[0, 0, \\frac{\\pi}{6}\\right]^T$ and $\\mathbf{x}_{t} = \\left[0.2, 0.1, \\frac{11\\pi}{60}\\right]^T$. Compute the odometry data $\\Delta \\mathbf{x}$ that results in this transformation.\n",
    "\n",
    "**ii**. Now suppose you received the odometry data $\\Delta\\mathbf{x}$ from part **i**, but your car was previously at position $\\mathbf{x}_{t-1} = \\left[3, 4, \\frac{\\pi}{3}\\right]^T$. Compute the current pose $\\mathbf{x}_t$.\n",
    "\n",
    "If you were to use this deterministic motion model in your particle filter all of your particles would end up in the same place - which defeats the purpose of having so many. When you build your actual motion model, you will be injecting noise into the function $g$ which will make your particles spread out as the car moves. This accounts for any uncertainty that exists in the odometry information. The sensor model will collapse this spreading of particles back into a small region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question2'></a>\n",
    "**Question 2**. (**Sensor Model**) The sensor model, $p\\left(z_{t}| x_{t}, m\\right)$, defines how likely it is to record a given sensor reading $z_{t}$ from a hypothesis position $x_{t}$ in a known, static map $m$ at time $t$. We will use this likelihood to \"prune\" the particles as the motion model tries to spread them out. Particles will be kept if the sensor reading is consistent with the map from the point of view of that particle. The definition of this likelihood is strongly dependent on the type of sensor used - a laser scanner in our case.\n",
    "\n",
    "In a lidar model, typically, there are a few cases to be modeled in determining $p\\left(z_{t}| x_{t}, m\\right)$:\n",
    "\n",
    "1. Probability of detecting a known obstacle in the map\n",
    "2. Probability of a short measurement. Maybe due to internal lidar reflections (scratches or oils on the surface), hitting parts of the vehicle itself, or other unknown obstacles (people, cats, etc).\n",
    "3. Probability of a very large (aka missed) measurement. Usually due to lidar beams that hit an object with strange reflective properties and did not bounce back to the sensor\n",
    "4. Probability of a completely random measurement. Just in case of an asteroid strike.\n",
    "    \n",
    "We typically represent (1) with a gaussian distribution centered around the ground truth distance between the hypothesis pose and the nearest map obstacle. Thus, if the measured range exactly matches the expected range, the probability is maximum. If the measured range is $z_{t}$ and the ground truth range is determined (via ray casting on the map $m$) to be $z_{t}^*$ then we have that:\n",
    "\n",
    "$$\n",
    "\tp_{hit}(z_{t}| x_{t}, m)  = \n",
    "\t\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_t - z_t^*)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\t\n",
    "Case (2) is represented as a downward sloping line as the ray gets further from the robot.\n",
    "This is because if the unknown obstacles (people cats, etc.) are distributed uniformly in the environment, the lidar is more likely to hit ones that are closer (think about how area scales...). This likelihood can be modeled as:\n",
    "\n",
    "$$\n",
    "\tp_{short}\\left(z_{t}| x_{t}, m\\right) =  \\frac{2}{z_t^*} \\begin{cases}\n",
    "         1 - \\frac{z_{t}}{z_{t}^*}   &   \\text{if} \\quad 0 \\leq z_{t}^{k} \\leq z_{t}^*\\\\\n",
    "         0   &   \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Case (3) is represented by a large spike in probability at the maximal range value, so that reflected measurements do not significantly discount particle weights.\n",
    "\n",
    "$$\n",
    "\tp_{max}(z_{t}| x_{t}, m)  =\\begin{cases}\n",
    "\t1  &  \\text{if} \\quad z_t = z_{max}\\\\\n",
    "\t0  &  \\text{otherwise} \n",
    "\t\\end{cases}\n",
    "\t$$\n",
    "\n",
    "Case (4) is represented by a small uniform value, to account for unforeseen effects.\n",
    "\n",
    "$$\n",
    "\tp_{rand}(z_{t}| x_{t}, m)  = \\begin{cases}\n",
    "\t\\frac{1}{z_{max}}  &  \\text{if} \\quad 0\\leq z_{t} \\leq z_{max}\\\\\n",
    "\t0                            & \\text{otherwise} \n",
    "\t\\end{cases}\n",
    "$$\n",
    "\t\n",
    "These four different distributions are now mixed by a weighted average, defined by the parameters $\\alpha_{hit}, \\alpha_{short},\\alpha_{max},\\alpha_{rand}$:\n",
    "\n",
    "$$\n",
    "\t p(z_{t}| x_{t}, m)  = \\alpha_{hit} \\cdot p_{hit}(z_{t}| x_{t}, m)  + \\alpha_{short} \\cdot p_{short}(z_{t}| x_{t}, m)  + \\alpha_{max} \\cdot p_{max}(z_{t}| x_{t}, m)  + \\alpha_{rand} \\cdot p_{rand}(z_{t}| x_{t}, m) \n",
    "$$\n",
    "\n",
    "Note that in order for $p(z_{t}| x_{t}, m)$ to be a probability distrubution we have have that:\n",
    "\n",
    "$$\n",
    "\\alpha_{hit}+\\alpha_{short}+\\alpha_{max}+\\alpha_{rand}=1\n",
    "$$\n",
    "\n",
    "All together this should look like this:\n",
    "\n",
    "<img src=\"figures/sensor_model_slice2.png\" width=\"600\">\n",
    "\t \n",
    "Find the values of $p(z_{t} | x_{t}, m)$ for the values of $z_t$ below and the following parameters:\n",
    "$\\alpha_{hit} = 0.8$,\n",
    "$\\alpha_{short}=0.01$,\n",
    "$\\alpha_{max}=0.07$,\n",
    "$\\alpha_{rand}=0.12$,\n",
    "$\\sigma=0.5\\text{ m}$\n",
    "$z_{max}=10\\text{ m}$, and\n",
    "$z_{t}^{*} = 7\\text{ m}$.\n",
    "\n",
    "**i**.    $z_{t} = 0\\text{ m}$\n",
    "\n",
    "**ii**.   $z_{t} = 3\\text{ m}$\n",
    "\n",
    "**iii**.  $z_{t} = 5\\text{ m}$\n",
    "\n",
    "**iv**.   $z_{t} = 8\\text{ m}$\n",
    "\n",
    "**v**.    $z_{t} = 10\\text{ m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Programming Assignment\n",
    "### 1. Getting Started\n",
    "Grab (fork) the skeleton code from this repository. In this lab, you will use RangeLibc for super fast ray casting. Check out [the repo](https://github.com/kctess5/range_libc) if for some reason you don’t already have it installed. An algorithm overview and usage instructions can be found here: [RangeLibc documentation](https://docs.google.com/document/d/1NSRAAZhNbmrsyQynYBgQuT2EUYupjg3Qafu1OtkTYtc/export?format=pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Motion Model\n",
    "While you could use the IMU in the motion model, we recommend using the wheel odometry coming from dead-reckoning integration of motor and steering commands. It will probably be less noisy than the IMU except in extreme operating conditions. You may assume that the motor commands relate to the movement of your car according to the [geometric Ackermann model](http://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf), with some noise due to tire slip and other imperfections. The `/vesc/odom` topic does this for you already on the car!\n",
    "\n",
    "The precise definition of your motion model is up to you. The main constraints are:\n",
    "\n",
    "- You must add random noise! Otherwise, as mentioned earlier, the particles won't spread out.\n",
    "- As the noise approaches zero, the motion model should become exactly equal to the deterministic model you worked with in the writing section.\n",
    "\n",
    "You might empirically determine your noise coefficients based on what works, or could try to gather data from the car which allows you to directly determine your measurement uncertainty. You will also have to model how your available odometry data (i.e. steering commands speed and steering) correspond to changes in states $(x,y,\\theta)$.\n",
    "\n",
    "We recommend you start with the formulas you derived in [Question 1 of part A](#question1) and think of ways to add noise to these equations in a way that makes sense geometrically. The algorithm ```sample_motion_model_odometry``` in the [Probabilistic Robotics textbook](https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf) may also help although it is not necessarily accurate to the car's geometry. All of Chapter 5 is a great resource to understand this lab's theory a little more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Sensor Model\n",
    "Once you have updated particle positions via your motion model, you use the sensor model to assign likelihood weights to each particle. This allows good hypotheses to have better likelihood of being sampled on the next iteration, and visa versa for bad hypotheses. \n",
    "\n",
    "\n",
    "#### Precomputing the model\n",
    "\n",
    "You may have noticed while doing the writing section that the sensor model is quite complicated and requires a lot of multiplications and non-trivial functions. And in the end this function is something that is \"guessed\". It will never reflect the true distribution and so long as it has the general shape that is good enough. So rather than evaluating this function millions of times at runtime, you can evaluate it on a discretized grid of inputs $z_t$ and $z_t^*$ and store these values in a lookup table.\n",
    "\n",
    "This strategy is described in section 3.4.1 of [2]. The motivation is twofold:\n",
    "\n",
    "1. Reading off a probability value from a table based on your actual sensor measurement and raycast ‘ground truth’ values is **MUCH** faster than doing all the math every time;\n",
    "2. This provides a good opportunity to numerically normalize the $p(z_{t} |x_t, m)$ function (make sure your probabilities sum to 1).\n",
    "\n",
    "Here’s some pseudocode for how this might look in your implementation:  \n",
    "\n",
    "```python\n",
    "def precompute_sensor_model()\n",
    "    # Experimentally determine some constants in the sensor model\n",
    "    # create a lookup table\n",
    "    return sensor_model_table\n",
    "\n",
    "sensor_model_table = precompute_sensor_model()  \n",
    "def sensor_model_table_lookup(r, d):  \n",
    "    '''\n",
    "    Get the probability of measured distance r given groundtruth distance d from precomuted table\n",
    "    ''' \n",
    "    return sensor_model_table[discretize(r), discretize(d)] \n",
    "```\n",
    "\n",
    "And your sensor model might look something like:\n",
    "\n",
    "```python\n",
    "range_method = range_lib.\"SomeRangeMethod\"(map_msg, max_range_in_pixels)\n",
    "\n",
    "def sensor_model(px,py,theta,ranges):\n",
    "    '''\n",
    "    Find the probablistic weight for each particle in the proposal distribution\n",
    "    \n",
    "    This sensor model is defined for a single particle (px,py,theta)  \n",
    "    however, in practice, you will find it more efficient to define your  \n",
    "    sensor model for the entire array of particles instead \n",
    "    \n",
    "    Args:\n",
    "        px, py, theta: pose of the particle\n",
    "        ranges: measured ranges from LiDAR\n",
    "    '''\n",
    "    # queries is the set of (x,y,theta) poses to ray cast from \n",
    "    queries = make_query_states(px, py, theta)  \n",
    "    groundtruth_ranges = np.zeros(queries.shape[0])  \n",
    "    range_method.calc_range_many(queries,groundtruth_ranges)  \n",
    "    # compute weight\n",
    "    partial_weights = map(lambda r,d: sensor_model_table_lookup(r,d), ranges, groundtruth_ranges)  \n",
    "    weight = product(partial_weights)  \n",
    "    # do other things to the weight...\n",
    "    return weight\n",
    "\n",
    "```\n",
    "\n",
    "The above pseudocode for a sensor model is unnecessarily slow - think about how it could be improved. See below, and the documentation for RangeLibc for further discussion.\n",
    "<img src=\"figures/sensor_model2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Particle Resampling\n",
    "In the resampling step of your particle filter, you should randomly draw from the set of particles according to the probability weights computed in the previous iteration:\n",
    "$$\n",
    "    \\mathbf{x}_{t-1}^{k} \\sim \\mathbf{X}_{t-1}\n",
    "$$\n",
    "This set is called the proposal distribution. Notice: you will very likely (à la [birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem)) draw the same particle more than once, but this is ok because **your motion model incorporates randomness** that will ensure we don’t evaluate the observation model with several copies of the same particle.\n",
    "\n",
    "You may find the [numpy.random.choice](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) function useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Particle Filter\n",
    "Once you have defined your motion and sensor models, you should implement the recursive algorithm something like: \n",
    "\n",
    "```python\n",
    "def MCL(X_prev, u_prev, z_cur):  \n",
    "    X_cur = {}  \n",
    "    for i in range(m):  \n",
    "        # sample a pose from the old particles according to old weights.  \n",
    "        # Samples implicitly represent the prior prob. dist. Bel(x_prev)  \n",
    "        x_prev = resample(X_prev)   \n",
    "        # update the sampled pose according to the motion model \n",
    "        x_cur = motion_model(x_prev, u_prev) \n",
    "        # weight the updated pose according to the sensor model   \n",
    "        w_cur = sensor_model(x_cur, z_cur) \n",
    "        # add the new pose and weight to the new distribution \n",
    "        X_cur = update(x_cur, w_cur)  \n",
    "    # normalize weights, should sum to 1 \n",
    "    X_cur = normalize_importance_weight(X_cur)  \n",
    "    return X_cur\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "def particle_filter():  \n",
    "    X = Bel(x0) ← initial particles   \n",
    "    while True:  \n",
    "        u = get_last_odometry()    \n",
    "        z = get_last_sensor_readings()   \n",
    "        X = MCL(X, u, z)  \n",
    "        # inferred pose ← expected value over particle distribution \n",
    "        pose = expectation(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Test your implementation in simulation**.\n",
    " Similar to previous labs, we have provided an autograder to test your particle filter. The autograder will launch the following nodes:\n",
    "-       `racecar/launch/teleop.launch` (temporarily at startup)\n",
    "-   \t`localization/launch/map_server.launch`\n",
    "-   \t`localization/launch/localize.launch`\n",
    "    \n",
    "\n",
    "Once the autograder detects your particle filter is running (as soon as it receives a message from `/pf/pose/odom`) it will launch a rosbag collected on a Hokuyo flavored RACECAR while driving around the basement. Then, the autograder receives messages published on `/pf/pose/odom` and extracts (time, x, y, theta) information from each message via `Odometry.pose.pose.position`, `quaternion_to_angle` (`Odometry.pose.pose.orientation`) and `Odometry.header.stamp`. When the test completes (or is terminated early) the autograder will output a `log.npz` file that contains this trajectory data, which you need to upload to Gradescope. You should all submit the log.npz file, but you can share that file among your own team.\n",
    "\n",
    "Periodically throughout testing, we provide initialization via the `/initialpose `topic, so it’s extremely important that you implement this functionality in your solution. You are not allowed to manually provide your own reinitialization messages during testing.\n",
    "\n",
    "You may run the test at less than real time via the `--rate` argument (e.g. `./run_tests --rate 0.5`). However, the grade you receive will also be scaled by this quantity according to the below chart, so it is in your interest to run at real time! For fairness, we require that all teams run the test scripts on their car. This will avoid potentially penalizing people with slower laptops.\n",
    "\n",
    "FYI, TA's solution generally gets $>0.9$ points on Gradescope. Note: the bit where the car is by the bikes is tricky! The map isn’t perfect, and therefore you will not achieve perfection there. Don’t worry about it, everyone will have the same problem.\n",
    "\n",
    "Note: you will have to install the map server node on the RACECAR.\n",
    "\n",
    "\tsudo apt-get install ros-kinetic-map-server\n",
    "    \n",
    "<img src=\"figures/scaling.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Notes and Tips\n",
    "If you implement the particle filter exactly as described above, you will likely discover that it has very poor performance. Largely, this poor performance is due to assumptions made during the derivation of the particle filter which are not strictly true. \n",
    "\n",
    "For example, the particle distribution approximates the true distribution in the limit - with an infinite number of particles. Clearly this is problematic since we maintain at max a few thousand particles. You should take a look at [4,5] for some useful tips for improving real world problems. \n",
    "\n",
    "You will want to downsample the number of range measurements from the particle filter from >1000 to less than 100. This will make the probability distribution over your state space less “peaked” and increase the number of particles you can maintain in real time (less ray casting). Additionally, you will probably want to “squash” your sensor model output probability by raising it to a power of less than one (1/3 for example) to make your distribution even less peaked. If your are confused by this paragraph, look at [4,5]. \n",
    "Similarly, you may notice that the TA sensor model table shown above is not as peaked as you might expect given the LiDAR accuracy (namely the “hit” term in [question 2 of part A](#question2)) . This is largely to smooth the probability distribution over the particle state space so that the convergence basin of the algorithm is larger.\n",
    "\n",
    "Here are more tips on how to improve your code:\n",
    "\n",
    "#### 4.1 Writing efficient Python\n",
    "Since the algorithm must run at >20Hz with a large number of particles, an efficient implementation is a requirement for success. There are a few tricks you can use, primarily:\n",
    "-   Use numpy arrays for absolutely everything - python lists → slow\n",
    "\n",
    "-   Use numpy functions on numpy arrays to do any computations, \n",
    "    - avoid Python for loops like the plague\n",
    "    -   [Slice indexing is your (best) friend.](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)\n",
    "    \n",
    "-   Use the smallest number of operations required to perform your arithmetic\n",
    "\n",
    "-   Avoid unnecessary memory allocations\n",
    "    -   Cache and reuse important numpy arrays by setting them to the right size during initialization of your particle filter as “self” variables\n",
    "    -   Be careful! Avoid bugs due to stale state in reused variables!\n",
    "    \n",
    "-   Identify your critical code paths, and keep them clean\n",
    "    - Conversely, don’t worry too much about code that is called infrequently\n",
    "    \n",
    "-   Push code to Cython/C++ if necessary\n",
    "    -   You probably won’t need to do this much since RangeLibc already does this\n",
    "    \n",
    "-   Avoid excessive function calls - function call overhead in Python → slow\n",
    "\n",
    "-   Use RangeLibc\n",
    "    -   saveTrace with Bresenham’s Line can be useful for debugging (see docs)\n",
    "\n",
    "-   Don’t publish visualization messages unless someone is subscribed to those topics\n",
    "```\n",
    "    if  self.[some_pub].get_num_connections() >  0 …\n",
    "```\n",
    "-   USE A PROFILER to identify good candidates for optimization\n",
    "    -   [http://projects.csail.mit.edu/pr2/wiki/index.php?title=Profiling_Code_in_ROS](http://projects.csail.mit.edu/pr2/wiki/index.php?title=Profiling_Code_in_ROS)\n",
    "-   Use rostopic hz to determine how fast your nodes are publishing.\n",
    "    -   If your particle filter is publishing the pose significantly slower than the laser scan than there is a lot of lag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 RangeLibc\n",
    "\n",
    "Assuming a reasonably efficient architecture, you will find that the bottleneck computation in running your particle filter is the determination of ground truth ranges between hypothesis poses (particles) and the nearest obstacles in the map - ray casting. Rather than having you implement the “calc_range” function yourself in Python, we provide the RangeLibc library to enable real time particle filter operation. See [RangeLibc document](https://docs.google.com/document/d/1NSRAAZhNbmrsyQynYBgQuT2EUYupjg3Qafu1OtkTYtc/export?format=pdf) for an algorithm overview and usage instructions. We highly recommend you read through it since it includes tips to make your sensor model implementations significantly simpler and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 RViz is your friend\n",
    "\n",
    "-   Visual inspection is the best debugging tool\n",
    "    \n",
    "-   Significantly better than looking at a stream of numbers in a terminal\n",
    "    \n",
    "-   Add visualization early and often\n",
    "    \n",
    "-   Subscribe to the /initialpose or /clicked_point topics and use the associated tools in RViz (“2D Pose Estimate”/”Clicked Point”) to initialize or reinitialize your particle filter [(see video example of this)](https://www.youtube.com/watch?v=IQ1GtNJvBjg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Debugging tips\n",
    "\n",
    "#### Motion model\n",
    "\n",
    "1.  Disable your sensor model update\n",
    "    \n",
    "2.  Disable your motion model noise\n",
    "    \n",
    "3.  Ensure that when you move the car, the particles move in the same manner.\n",
    "    \n",
    "4.  Rotate your car but do not reinitalize your particles - once again drive around and ensure your particles do something reasonable\n",
    "    \n",
    "5.  Once you’re happy with the results from the above tests, enable your motion model noise. Now, you should see the particles spreading out as you drive around, but they should all move in roughly the right direction without excessive spread.\n",
    "    \n",
    "6.  Enable your sensor model now and see that the particle distribution ‘collapses’ around the correct pose after each sensor measurement is considered\n",
    "    \n",
    "\n",
    "#### Sensor model\n",
    "\n",
    "1. Print your probabilities and look at them. The sensor model is prone to some numerical issues because multiplying a bunch of very small numbers leads to extremely small values which can exceed float precision. If this happens, you might be able to think of ways to rearrange your order of operations to avoid these problems.\n",
    "    \n",
    "2. Evaluate your model with custom arguments\n",
    "    \n",
    "  - Providing the same ‘observation’ and ‘ground truth’ scans should result in reasonably large probabilities\n",
    "    \n",
    "  - Providing two extremely different scans should result in low probabilities\n",
    "    \n",
    "  - Etc - make up some test cases and ensure your expectations match reality\n",
    "    \n",
    "\n",
    "#### Ray casting with RangeLibc\n",
    "\n",
    "Try doing some simple cases of ray casting and make sure you get basically what you expect. You can (for example) dump the distance values you get from simulating a laser scan at a known pose, and plot them somehow. What you see in the plot should match your expectations, if not, maybe you have your x/y coordinates reversed, or some other problem like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Run on Real RACECAR\n",
    "1. Demonstrate the functionality of your algorithm on the real RACECAR; \n",
    "\n",
    "3. Check [Submission](#submission) for further instruction.\n",
    "\n",
    "We will announce the schedule for  the presentation in next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - (OPTIONAL) Simultaneous Localization and Mapping - Configure and Run Google Cartographer\n",
    "Configure and run **Google Cartographer** on the real RACECAR. [Cartographer](https://github.com/googlecartographer/cartographer) is a system that provides real-time simultaneous localization and mapping ([SLAM](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)) in 2D and 3D across multiple platforms and sensor configurations.  Please go to [this repository](https://github.com/mit-rss/cartographer_config), follow the instruction to configure and run cartographer on the real RACECAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. <a name=\"ThrunRobust\"></a>[S. Thrun, D. Fox, W. Burgard and F. Dellaert. “Robust Monte Carlo Localization for Mobile Robots.” Artificial Intelligence Journal. 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.6016&rep=rep1&type=pdf).\n",
    "2. <a name=\"FoxMarkov\"></a>[D. Fox, W. Burgard, and S. Thrun. “Markov localization for mobile robots in dynamic environments,” Journal of Artificial Intelligence Research, vol. 11, pp. 391427, 1999](http://www.jair.org/media/616/live-616-1819-jair.pdf).\n",
    "3. <a name=\"FoxAdvances\"></a>[D. Fox. “KLD-sampling: Adaptive particle filters,” Advances in Neural Information Processing Systems 14 (NIPS), Cambridge, MA, 2002. MIT Press](https://papers.nips.cc/paper/1998-kld-sampling-adaptive-particle-filters.pdf).\n",
    "4. <a name=\"BagnellPracticle\"></a> [D. Bagnell “Particle Filters: The Good, The Bad, The Ugly”\n",
    "](http://www.cs.cmu.edu/~16831-f12/notes/F14/16831_lecture05_gseyfarth_zbatts.pdf)\n",
    "5. <a name=\"BootsImportance\"></a>[B. Boots “Importance Sampling, Particle Filters”](https://web.archive.org/web/20170209092754/http://www.cc.gatech.edu/~bboots3/STR-Spring2017/Lectures/Lecture4/Lecture4_notes.pdf)\n",
    "6. <a name=\"WalshCDDT\"></a>[C. Walsh and S. Karaman “CDDT: Fast Approximate 2D Ray Casting for Accelerated Localization”](https://arxiv.org/abs/1705.01167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
